{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from irelease.utils import generate_smiles, get_default_tokens\n",
    "from irelease.data import GeneratorData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_hparams():\n",
    "    return {'d_model': 1500,\n",
    "            'dropout': 0.0,\n",
    "            'monte_carlo_N': 5,\n",
    "            'use_monte_carlo_sim': True,\n",
    "            'no_mc_fill_val': 0.0,\n",
    "            'gamma': 0.97,\n",
    "            'episodes_to_train': 10,\n",
    "            'gae_lambda': 0.95,\n",
    "            'ppo_eps': 0.2,\n",
    "            'ppo_batch': 1,\n",
    "            'ppo_epochs': 5,\n",
    "            'entropy_beta': 0.01,\n",
    "            'use_true_reward': args.use_true_reward,\n",
    "            'reward_params': {'num_layers': 2,\n",
    "                              'd_model': 512,\n",
    "                              'unit_type': 'gru',\n",
    "                              'demo_batch_size': 32,\n",
    "                              'irl_alg_num_iter': 5,\n",
    "                              'dropout': 0.2,\n",
    "                              'use_attention': args.use_attention,\n",
    "                              'use_validity_flag': ~args.no_smiles_validity_flag,\n",
    "                              'bidirectional': True,\n",
    "                              'optimizer': 'adadelta',\n",
    "                              'optimizer__global__weight_decay': 0.0005,\n",
    "                              'optimizer__global__lr': 0.001, },\n",
    "            'agent_params': {'unit_type': 'gru',\n",
    "                             'num_layers': 2,\n",
    "                             'stack_width': 1500,\n",
    "                             'stack_depth': 200,\n",
    "                             'optimizer': 'adadelta',\n",
    "                             'optimizer__global__weight_decay': 0.0000,\n",
    "                             'optimizer__global__lr': 0.001},\n",
    "            'critic_params': {'num_layers': 2,\n",
    "                              'd_model': 256,\n",
    "                              'dropout': 0.2,\n",
    "                              'unit_type': 'lstm',\n",
    "                              'optimizer': 'adadelta',\n",
    "                              'optimizer__global__weight_decay': 0.00005,\n",
    "                              'optimizer__global__lr': 0.001},\n",
    "            'expert_model_params': {'model_dir': './model_dir/expert_rnn_reg',\n",
    "                                    'd_model': 128,\n",
    "                                    'rnn_num_layers': 2,\n",
    "                                    'dropout': 0.8,\n",
    "                                    'is_bidirectional': False,\n",
    "                                    'unit_type': 'lstm'}\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_provider():\n",
    "    tokens = get_default_tokens()\n",
    "    demo_data = GeneratorData(training_data_path='../../data/logp_smiles_biased.smi',\n",
    "                              delimiter='\\t',\n",
    "                              cols_to_read=[0],\n",
    "                              keep_header=True,\n",
    "                              pad_symbol=' ',\n",
    "                              max_len=120,\n",
    "                              tokens=tokens,\n",
    "                              use_cuda=use_cuda)\n",
    "    unbiased_data = GeneratorData(training_data_path='../../data/unbiased_smiles.smi',\n",
    "                                  delimiter='\\t',\n",
    "                                  cols_to_read=[0],\n",
    "                                  keep_header=True,\n",
    "                                  pad_symbol=' ',\n",
    "                                  max_len=120,\n",
    "                                  tokens=tokens,\n",
    "                                  use_cuda=use_cuda)\n",
    "    return {'demo_data': demo_data, 'unbiased_data': unbiased_data, 'prior_data': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(hparams, demo_data_gen, unbiased_data_gen, *args, **kwargs):\n",
    "    # Embeddings provider\n",
    "    encoder = Encoder(vocab_size=demo_data_gen.n_characters, d_model=hparams['d_model'],\n",
    "                      padding_idx=demo_data_gen.char2idx[demo_data_gen.pad_symbol],\n",
    "                      dropout=hparams['dropout'], return_tuple=True)\n",
    "\n",
    "    # Agent entities\n",
    "    rnn_layers = []\n",
    "    has_stack = True\n",
    "    for i in range(1, hparams['agent_params']['num_layers'] + 1):\n",
    "        rnn_layers.append(StackRNN(layer_index=i,\n",
    "                                   input_size=hparams['d_model'],\n",
    "                                   hidden_size=hparams['d_model'],\n",
    "                                   has_stack=has_stack,\n",
    "                                   unit_type=hparams['agent_params']['unit_type'],\n",
    "                                   stack_width=hparams['agent_params']['stack_width'],\n",
    "                                   stack_depth=hparams['agent_params']['stack_depth'],\n",
    "                                   k_mask_func=encoder.k_padding_mask))\n",
    "        if hparams['agent_params']['num_layers'] > 1:\n",
    "            rnn_layers.append(StackedRNNDropout(hparams['dropout']))\n",
    "            rnn_layers.append(StackedRNNLayerNorm(hparams['d_model']))\n",
    "    agent_net = nn.Sequential(encoder,\n",
    "                              *rnn_layers,\n",
    "                              StackRNNLinear(out_dim=demo_data_gen.n_characters,\n",
    "                                             hidden_size=hparams['d_model'],\n",
    "                                             bidirectional=False,\n",
    "                                             bias=True))\n",
    "    with contextlib.suppress(Exception):\n",
    "        agent_net = agent_net.to(device)\n",
    "    optimizer_agent_net = parse_optimizer(hparams['agent_params'], agent_net)\n",
    "    selector = MolEnvProbabilityActionSelector(actions=demo_data_gen.all_characters)\n",
    "    probs_reg = StateActionProbRegistry()\n",
    "    init_state_args = {'num_layers': hparams['agent_params']['num_layers'],\n",
    "                       'hidden_size': hparams['d_model'],\n",
    "                       'stack_depth': hparams['agent_params']['stack_depth'],\n",
    "                       'stack_width': hparams['agent_params']['stack_width'],\n",
    "                       'unit_type': hparams['agent_params']['unit_type']}\n",
    "    agent = PolicyAgent(model=agent_net,\n",
    "                        action_selector=selector,\n",
    "                        states_preprocessor=seq2tensor,\n",
    "                        initial_state=agent_net_hidden_states_func,\n",
    "                        initial_state_args=init_state_args,\n",
    "                        apply_softmax=True,\n",
    "                        probs_registry=probs_reg,\n",
    "                        device=device)\n",
    "    critic = nn.Sequential(encoder,\n",
    "                           CriticRNN(hparams['d_model'], hparams['critic_params']['d_model'],\n",
    "                                     unit_type=hparams['critic_params']['unit_type'],\n",
    "                                     dropout=hparams['critic_params']['dropout'],\n",
    "                                     num_layers=hparams['critic_params']['num_layers']))\n",
    "    with contextlib.suppress(Exception):\n",
    "        critic = critic.to(device)\n",
    "    optimizer_critic_net = parse_optimizer(hparams['critic_params'], critic)\n",
    "    drl_alg = PPO(actor=agent_net, actor_opt=optimizer_agent_net,\n",
    "                  critic=critic, critic_opt=optimizer_critic_net,\n",
    "                  initial_states_func=agent_net_hidden_states_func,\n",
    "                  initial_states_args=init_state_args,\n",
    "                  device=device,\n",
    "                  gamma=hparams['gamma'],\n",
    "                  gae_lambda=hparams['gae_lambda'],\n",
    "                  ppo_eps=hparams['ppo_eps'],\n",
    "                  ppo_epochs=hparams['ppo_epochs'],\n",
    "                  ppo_batch=hparams['ppo_batch'],\n",
    "                  entropy_beta=hparams['entropy_beta'])\n",
    "\n",
    "    # Reward function entities\n",
    "    reward_net = nn.Sequential(encoder,\n",
    "                               RewardNetRNN(input_size=hparams['d_model'],\n",
    "                                            hidden_size=hparams['reward_params']['d_model'],\n",
    "                                            num_layers=hparams['reward_params']['num_layers'],\n",
    "                                            bidirectional=hparams['reward_params']['bidirectional'],\n",
    "                                            use_attention=hparams['reward_params']['use_attention'],\n",
    "                                            dropout=hparams['reward_params']['dropout'],\n",
    "                                            unit_type=hparams['reward_params']['unit_type'],\n",
    "                                            use_smiles_validity_flag=hparams['reward_params']['use_validity_flag']))\n",
    "    with contextlib.suppress(Exception):\n",
    "        reward_net = reward_net.to(device)\n",
    "    expert_model = RNNPredictor(hparams['expert_model_params'], device)\n",
    "    reward_function = RewardFunction(reward_net, mc_policy=agent, actions=demo_data_gen.all_characters,\n",
    "                                     device=device, use_mc=hparams['use_monte_carlo_sim'],\n",
    "                                     mc_max_sims=hparams['monte_carlo_N'],\n",
    "                                     expert_func=expert_model,\n",
    "                                     use_true_reward=hparams['use_true_reward'],\n",
    "                                     true_reward_func=get_logp_reward,\n",
    "                                     no_mc_fill_val=hparams['no_mc_fill_val'])\n",
    "    optimizer_reward_net = parse_optimizer(hparams['reward_params'], reward_net)\n",
    "    demo_data_gen.set_batch_size(hparams['reward_params']['demo_batch_size'])\n",
    "    irl_alg = GuidedRewardLearningIRL(reward_net, optimizer_reward_net, demo_data_gen,\n",
    "                                      k=hparams['reward_params']['irl_alg_num_iter'],\n",
    "                                      agent_net=agent_net,\n",
    "                                      agent_net_init_func=agent_net_hidden_states_func,\n",
    "                                      agent_net_init_func_args=init_state_args,\n",
    "                                      device=device)\n",
    "\n",
    "    init_args = {'agent': agent,\n",
    "                 'probs_reg': probs_reg,\n",
    "                 'drl_alg': drl_alg,\n",
    "                 'irl_alg': irl_alg,\n",
    "                 'reward_func': reward_function,\n",
    "                 'gamma': hparams['gamma'],\n",
    "                 'episodes_to_train': hparams['episodes_to_train'],\n",
    "                 'expert_model': expert_model,\n",
    "                 'demo_data_gen': demo_data_gen,\n",
    "                 'unbiased_data_gen': unbiased_data_gen,\n",
    "                 'gen_args': {'num_layers': hparams['agent_params']['num_layers'],\n",
    "                              'hidden_size': hparams['d_model'],\n",
    "                              'num_dir': 1,\n",
    "                              'stack_depth': hparams['agent_params']['stack_depth'],\n",
    "                              'stack_width': hparams['agent_params']['stack_width'],\n",
    "                              'has_stack': has_stack,\n",
    "                              'has_cell': hparams['agent_params']['unit_type'] == 'lstm',\n",
    "                              'device': device}}\n",
    "    return init_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['demo_data', 'unbiased_data', 'prior_data'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generators = data_provider()\n",
    "generators.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_els = initialize(default_hparams(),generators['demo_data'], generators['unbiased_data'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
